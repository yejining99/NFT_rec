{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset\n",
    "This code preprocesses the raw data of transaction interactions, item features, and user features that we collected from four NFT collections into a format that can be inputted into the model. The preprocessed data has been uploaded to Google Drive. The main points are as follows:\n",
    "- 1. **User filtering**: Filter only users who have made at least 5 transactions.\n",
    "- 2. **Temporal user split**: Randomly sample 40% of each user's interactions to use as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Select NFT collection name from ['bayc', 'coolcats', 'doodles', 'meebits'], for data pre-processing\n",
    "'''\n",
    "\n",
    "COLLECTION = 'bayc'\n",
    "\n",
    "df_collection = pd.read_csv(f\"dataset/transactions/{COLLECTION}.csv\")\n",
    "# set save_path if not exist\n",
    "save_path = 'dataset/collections/'+COLLECTION+'/'\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of interactions before filtering: 29972\n",
      "Number of interactions after filtering: 29529\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data preprocessing (filtering)\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Number of interactions before filtering: {len(df_collection)}\")\n",
    "\n",
    "# 1) drop duplicated interactions (i.e., drop rows that Buyer and Token ID are identical)\n",
    "# df_collection = df_collection.drop_duplicates(subset=['Buyer', 'Token ID'], keep='first')\n",
    "\n",
    "# 2) Exclude items that we do not have features data for.\n",
    "#   That is, Only items that exist in the item features file will be left.\n",
    "#   For reference, 'price' and 'transaction' files include only items that were first traded before 2023.\n",
    "image = pd.read_csv(f'dataset/features_item/{COLLECTION}_img.csv', index_col=0)\n",
    "text = pd.read_csv(f'dataset/features_item/{COLLECTION}_txt.csv', index_col=0)\n",
    "price = pd.read_csv(f'dataset/features_item/{COLLECTION}_prices.csv', index_col=0)\n",
    "transaction = pd.read_csv(f'dataset/features_item/{COLLECTION}_txns.csv', index_col=0)\n",
    "indices = set(image.index).intersection(set(text.index)).intersection(set(price.index)).intersection(set(transaction.index))\n",
    "df_collection = df_collection[df_collection['Token ID'].isin(indices)]\n",
    "\n",
    "# 3) Exclude users that we do not have features data for.\n",
    "#   That is, Only users that exist in the user features file will be left.\n",
    "df_feature = pd.read_csv('dataset/features_user/user_features.csv', index_col=0) #.drop(['Unnamed: 0'], axis=1)\n",
    "# leave only user that exist in the df_feature 'Buyer' column\n",
    "df_collection = df_collection[df_collection['Buyer'].isin(df_feature['Buyer'])]\n",
    "\n",
    "print(f\"Number of interactions after filtering: {len(df_collection)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## interactions ('inter')\n",
    "- input\n",
    "    - NFT transactions data in 'transactions' folder, collected from Etherscan NFT tracker (https://etherscan.io/nfttracker)\n",
    "- output\n",
    "    - An .npy formatted interaction file (user, item, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transactions before USER CUT: 29529\n",
      "Number of transactions before USER CUT: 13763\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "USER_CUT: Only users that have been traded at least CUT times will be used.\n",
    "\"\"\"\n",
    "CUT = 5\n",
    "\n",
    "# print len of df_collection\n",
    "print(f\"Number of transactions before USER CUT: {len(df_collection)}\")\n",
    "\n",
    "# get the list of \"Buyer\" whose count is more than 3\n",
    "user_count = df_collection['Buyer'].value_counts()\n",
    "user_count = user_count[user_count >= CUT]\n",
    "user_count = user_count.index.tolist()\n",
    "\n",
    "# drop rows whose \"Buyer\" is not in user_count\n",
    "df_collection = df_collection[df_collection['Buyer'].isin(user_count)]\n",
    "\n",
    "# print len of df_collection\n",
    "print(f\"Number of transactions before USER CUT: {len(df_collection)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user length:  1230\n",
      "item length:  6726\n",
      "inter length:  13737\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate price labels, for later use of multi-objectives training\n",
    "\"\"\"\n",
    "\n",
    "# drop rows where 'Price' does not contain '$'\n",
    "df_collection = df_collection[df_collection['Price'].str.contains(\"\\$\")]\n",
    "# convert 'Price' to the value before 'ETH'\n",
    "df_collection['Price'] = df_collection['Price'].apply(lambda x: x.split(' ')[2][2:-1].replace(',', '').replace('.', ''))\n",
    "df_collection['Price'] = df_collection['Price'].astype(float)\n",
    "\n",
    "# create a new variable 'Price_diff' which is the difference between the future price and the current price \n",
    "# get price differences from the same 'Token ID'\n",
    "df_collection['Price_diff'] = df_collection.groupby('Token ID')['Price'].diff(-1)\n",
    "# convert rows where 'Price_diff' is NaN into 0\n",
    "df_collection['Price_diff'] = df_collection['Price_diff'].fillna(0)\n",
    "# put minus to Price_diff\n",
    "df_collection['Price_diff'] = df_collection['Price_diff'].apply(lambda x: -x)\n",
    "# convert 'Price_diff' to 1 if the value is greater than 0, otherwise 0\n",
    "df_collection['Price_diff'] = df_collection['Price_diff'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# create an np.array with 'Buyer'\n",
    "user = df_collection['Buyer'].values\n",
    "item = df_collection['Token ID'].values\n",
    "labels = df_collection['Price_diff'].values\n",
    "data = (user, item, labels)\n",
    "\n",
    "# save as npy file\n",
    "np.save(save_path + f'{COLLECTION}.npy', data)\n",
    "\n",
    "# print user length and item length\n",
    "print('user length: ', len(set(user)))\n",
    "print('item length: ', len(set(item)))\n",
    "print('inter length: ', len(labels))\n",
    "\n",
    "# save user length and item length as a dictionary\n",
    "dict = {'num_user': len(set(user)), 'num_item': len(set(item))}\n",
    "np.save(save_path + 'num_user_item.npy', dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "*For RecBole*\n",
    "To use the same train, validation, and test sets when conducting baseline model experiments in RecBole, index information is stored.\n",
    "\"\"\"\n",
    "\n",
    "# save df_collection as csv file\n",
    "recbole_path = '/home/felab1/workspace/LEE/RecBole/dataset/transactions/'\n",
    "df_collection.reset_index(drop=True).to_csv(recbole_path + f'{COLLECTION}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of interactions:  (15708, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7389,  831,    0],\n",
       "       [6959, 1532,    1],\n",
       "       [7677,  832,    1],\n",
       "       ...,\n",
       "       [6874, 4276,    0],\n",
       "       [6957, 6224,    0],\n",
       "       [7893, 3510,    0]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ensure that the indices for the user and item do not overlap with each other.\n",
    "We map indices using dict where the key is the original index and the value is the new index.\n",
    "\n",
    "We map the item indices to the range of [0, len(set(item))).\n",
    "We add len(set(item)) to the user indices.\n",
    "For example,\n",
    "    Before:\n",
    "        item: [5, 6, 8. 9, 10, 13, 15, 20, 21, 29]\n",
    "        user: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    After:\n",
    "        item: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        user: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 1) Map the item idx: start from 0.\n",
    "item_unique = np.unique(item)\n",
    "mapping_i = {}\n",
    "for i in range(len(item_unique)):\n",
    "    mapping_i[item_unique[i]] = i\n",
    "\n",
    "# 2) Map the user idx: start from num_item.\n",
    "#   firstly, Change the user addresses to integers starting from 0 (e.g., 0x9137a5d195f0ab57e428c5a2be9bc8c4620445cb -> 0)\n",
    "#   then, add len(set(item)) to the user indices.\n",
    "user_unique = np.unique(user)\n",
    "mapping_u = {}\n",
    "for i in range(len(user_unique)):\n",
    "    mapping_u[user_unique[i]] = i + len(set(item))\n",
    "\n",
    "# 3) Create inter\n",
    "user_ = np.array([mapping_u[u] for u in user])\n",
    "item_ = np.array([mapping_i[i] for i in item])\n",
    "inter = np.array([user_, item_, labels]).T\n",
    "inter = inter.astype(np.int64)\n",
    "print('num of interactions: ', inter.shape)\n",
    "inter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user features (user_feat.npy)\n",
    "- input\n",
    "    - User features data in 'features_user' folder, collected and preprocessed from transactions file\n",
    "- output\n",
    "    - An .npy formatted user features file ('# of transactions', 'Avg transaction price', 'avg holding period')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_user:  1647\n"
     ]
    }
   ],
   "source": [
    "# read 'user features.csv'\n",
    "df_feature = pd.read_csv('dataset/features_user/user_features.csv', index_col=0) #.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "# scaling columns \"# of transactions\", \"Avg transaction price\", \"avg holding period\": MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "names = ['# of transactions', 'Avg transaction price', 'holding period']\n",
    "df_feature[names] = scaler.fit_transform(df_feature[names])\n",
    "\n",
    "# convert column 'Buyer' using mapping_u\n",
    "# if the value is not in mapping_u, remove the row\n",
    "df_feature['Buyer'] = df_feature['Buyer'].apply(lambda x: mapping_u[x] if x in mapping_u else np.nan)\n",
    "df_feature = df_feature.dropna()\n",
    "# convert column 'Buyer' to int\n",
    "df_feature['Buyer'] = df_feature['Buyer'].astype(int)\n",
    "print('num_user: ', len(df_feature))\n",
    "\n",
    "# set 'Buyer' as index\n",
    "df_feature = df_feature.set_index('Buyer')\n",
    "\n",
    "# save df as npy file\n",
    "np.save(save_path+'user_feat.npy', df_feature, allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data split (train.npy, val.npy, test.npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ratio: 63.48%\n",
      "Valid and Test ratio: 36.52%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "data split: use 40% of each user's interactions as validation and test data\n",
    "\"\"\"\n",
    "# for each user, a random transaction and create a separate dataset with them\n",
    "valid_and_test = []\n",
    "random_idx_list = []\n",
    "for u in np.unique(inter[:,0]):\n",
    "    num_sample = int(len(np.where(inter[:,0]==u)[0])*0.4) # 40% of the number of transactions\n",
    "    random_idx = np.random.choice(np.where(inter[:,0]==u)[0], num_sample, replace=False)\n",
    "    valid_and_test.extend(inter[random_idx])\n",
    "    random_idx_list.extend(random_idx)\n",
    "valid_and_test = np.array(valid_and_test)\n",
    "\n",
    "\"\"\"\n",
    "train\n",
    "\"\"\"\n",
    "# create a separate dataset where inter not in random_idx_list\n",
    "train = np.delete(inter, random_idx_list, axis=0)\n",
    "# get list of indices inter-random_idx_list\n",
    "train_idx_list = list(set(range(len(inter))) - set(random_idx_list))\n",
    "\n",
    "\"\"\"\n",
    "valid, test\n",
    "\"\"\"\n",
    "# split valid_and_test into valid and test\n",
    "# split random_idx_list into 5:5\n",
    "valid_idx_list, test_idx_list = train_test_split(random_idx_list, test_size=0.5, random_state=42)\n",
    "valid = inter[valid_idx_list]\n",
    "test = inter[test_idx_list]\n",
    "\n",
    "# get ratio of train/inter, in percentage\n",
    "print(f'Train ratio: {len(train)/len(inter)*100:.2f}%')\n",
    "print(f'Valid and Test ratio: {len(valid_and_test)/len(inter)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "*For RecBole*\n",
    "To use the same train, validation, and test sets when conducting baseline model experiments in RecBole, index information is stored.\n",
    "\"\"\"\n",
    "\n",
    "# create a list of lists, where each list contains indices of train, validation, and test sets\n",
    "indices = [train_idx_list, valid_idx_list, test_idx_list]\n",
    "\n",
    "# save indices as pkl file\n",
    "recbole_path = f'/home/felab1/workspace/LEE/RecBole/dataset/collections/{COLLECTION}/'\n",
    "with open(recbole_path + 'split_indices.pkl', 'wb') as f:\n",
    "    pickle.dump(indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7075, [283, 174]),\n",
       " (7370, [113, 4191]),\n",
       " (6789, [4853, 2111]),\n",
       " (7306, [2220, 6324]),\n",
       " (6833, [3878, 6238, 416, 419])]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "preprocessing valid data\n",
    "\"\"\"\n",
    "# using valid, create a dict where keys are unique users and values are items\n",
    "valid_dict = {}\n",
    "for i in range(len(valid)):\n",
    "    if valid[i][0] in valid_dict:\n",
    "        valid_dict[valid[i][0]].append(valid[i][1])\n",
    "    else:\n",
    "        valid_dict[valid[i][0]] = [valid[i][1]]\n",
    "\n",
    "# show the first five items in valid_dict\n",
    "list(valid_dict.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the item index in the order of the most traded (popular).\n",
    "\"\"\"\n",
    "\n",
    "# concat all values in valid_dict as a list\n",
    "valid_list = []\n",
    "for i in valid_dict.values():\n",
    "    valid_list += i\n",
    "\n",
    "# value count valid_list and sort values\n",
    "value_counts = pd.Series(valid_list).value_counts().sort_values(ascending=False)\n",
    "\n",
    "# extract indices of value_counts\n",
    "indices = value_counts.index\n",
    "\n",
    "# save indices as npy\n",
    "np.save(save_path+'indices_valid.npy', indices, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([7075, 283, 174]), list([7370, 113, 4191]),\n",
       "       list([6789, 4853, 2111]), list([7306, 2220, 6324]),\n",
       "       list([6833, 3878, 6238, 416, 419])], dtype=object)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert to the form required by the model\n",
    "e.g., 12656: [7314, 4820, 6304] -> list([12656, 7314, 4820, 6304])\n",
    "\"\"\"\n",
    "\n",
    "# Create an empty numpy array with dtype 'object'\n",
    "valid_array = np.empty(len(valid_dict), dtype=object)\n",
    "\n",
    "# Assign the lists directly to the elements of the array\n",
    "for i, (key, val) in enumerate(valid_dict.items()):\n",
    "    # include key in the list\n",
    "    valid_array[i] = [key] + val\n",
    "\n",
    "# show the first five items in valid_array\n",
    "valid_array[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6967, [6227, 1284]),\n",
       " (7560, [479, 3184, 1000, 3181]),\n",
       " (6928, [3063, 909]),\n",
       " (7538, [6371, 2008]),\n",
       " (6737, [1982])]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "preprocessing test data\n",
    "\"\"\"\n",
    "\n",
    "# using test, create a dict where keys are unique users and values are items\n",
    "test_dict = {}\n",
    "for i in range(len(test)):\n",
    "    if test[i][0] in test_dict:\n",
    "        test_dict[test[i][0]].append(test[i][1])\n",
    "    else:\n",
    "        test_dict[test[i][0]] = [test[i][1]]\n",
    "\n",
    "# show the first five items in test_dict\n",
    "list(test_dict.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the item index in the order of the most traded (popular).\n",
    "\"\"\"\n",
    "\n",
    "# concat all values in test_dict as a list\n",
    "test_list = []\n",
    "for i in test_dict.values():\n",
    "    test_list += i\n",
    "\n",
    "# value count test_list and sort values\n",
    "value_counts = pd.Series(test_list).value_counts().sort_values(ascending=False)\n",
    "\n",
    "# extract indices of value_counts\n",
    "indices = value_counts.index\n",
    "\n",
    "# save indices as npy\n",
    "np.save(save_path+'indices_test.npy', indices, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([6967, 6227, 1284]), list([7560, 479, 3184, 1000, 3181]),\n",
       "       list([6928, 3063, 909]), list([7538, 6371, 2008]),\n",
       "       list([6737, 1982])], dtype=object)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert to the form required by the model\n",
    "e.g., 12656: [7314, 4820, 6304] -> list([12656, 7314, 4820, 6304])\n",
    "\"\"\"\n",
    "\n",
    "# Create an empty numpy array with dtype 'object'\n",
    "test_array = np.empty(len(test_dict), dtype=object)\n",
    "\n",
    "# Assign the lists directly to the elements of the array\n",
    "for i, (key, val) in enumerate(test_dict.items()):\n",
    "    # include key in the list\n",
    "    test_array[i] = [key] + val\n",
    "\n",
    "# show the first five items in test_array\n",
    "test_array[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train, valid, test as npy file\n",
    "np.save(save_path+'train.npy', train, allow_pickle=True)\n",
    "np.save(save_path+'val.npy', valid_array, allow_pickle=True)\n",
    "np.save(save_path+'test.npy', test_array, allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adjacency matrix (adj_dict.npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6710, [2395, 1652, 6064, 4108, 381]), (7019, [6597, 56, 3208, 314, 3477, 1547, 1087, 3086, 5349, 3239, 1332, 6461]), (7150, [2368, 1798, 6532, 3313, 337, 10, 1692, 1693, 1069, 2969, 6657, 3430, 4603, 5094, 151, 4050, 3562, 1261, 5994, 1789, 883, 3514, 4029, 5517, 2165, 2212, 689, 1310, 4129, 1600, 2722, 3157, 6122, 4244, 3752, 205, 5301, 6230, 3170, 4178, 437, 2509, 5390, 2230, 3391, 5943, 121, 2772, 4592, 5047, 841, 3636, 13, 3603, 4048, 4980, 4912, 527, 5655, 5682, 38, 1969, 5023, 3391, 6649, 4980, 4570, 4923, 4668, 6225, 1602, 1295, 5850, 3081, 5654, 6331, 5476, 1617, 4003, 1824, 1649, 6497, 3529, 3073, 2782, 5763, 6166, 3382, 5115, 1721, 2468, 6387, 4569, 789, 3553, 5031, 2758, 820, 2823, 4440, 1093, 3423, 2162, 4555, 6177, 248, 4351, 6050, 2433, 6554, 6531, 2093, 1508, 4404, 2538, 1708, 5415, 1860, 2713, 2664, 6255, 2399, 1571, 4101, 2977, 6069, 470, 855, 328, 2987, 2586, 3812, 2310, 4513, 6155, 6577, 6416, 1108, 5473, 1111, 6356, 5669, 3998, 2478, 971, 482, 5654, 4340, 5839, 3762, 3554, 750, 165, 473, 5299, 3904, 38, 4425, 1765, 465, 2197, 6157, 4964, 1742, 936, 3771, 1634, 3891, 2203, 136, 1696, 4022, 2260, 6048, 6426, 3462, 206, 2321, 3948, 6205, 5710, 819, 390, 6593, 3183]), (7299, [6397, 566, 4663, 3234, 720, 284, 5448, 2293, 3069, 2814, 3114, 3457, 198, 1605, 1060, 5493, 1901, 2582, 2034, 504, 91, 6003, 3996]), (7282, [3916, 625, 5263, 3584, 187, 3318, 4810, 6189, 4410, 6559, 7])]\n"
     ]
    }
   ],
   "source": [
    "# first column of inter is user\n",
    "# second column of inter is item\n",
    "\n",
    "# create a dict where keys are user and values are items\n",
    "adj_dict = {}\n",
    "for i in range(len(inter)):\n",
    "    if inter[i][0] in adj_dict:\n",
    "        adj_dict[inter[i][0]].append(inter[i][1])\n",
    "    else:\n",
    "        adj_dict[inter[i][0]] = [inter[i][1]]\n",
    "\n",
    "# show the first five items in adj_dict\n",
    "print(list(adj_dict.items())[:5])\n",
    "\n",
    "# save adj_dict as npy file\n",
    "np.save(save_path+'adj_dict.npy', adj_dict, allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## item features (feat.npy)\n",
    "When using features, there is no need for tokenID to match inter because the index is used in features.\n",
    "\n",
    "- input\n",
    "    - Item features data in 'features_item' folder, collected and preprocessed from OpenSea\n",
    "- output\n",
    "    - An .npy formatted item features file (image, text, price, transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "image shape:  (20000, 1024)\n",
      "text shape:  (20000, 1800)\n",
      "price shape:  (9682, 64)\n",
      "transaction shape:  (9682, 64)\n",
      "\n",
      "After\n",
      "image shape:  (6693, 1024)\n",
      "text shape:  (6693, 1800)\n",
      "price shape:  (6693, 64)\n",
      "transaction shape:  (6693, 64)\n"
     ]
    }
   ],
   "source": [
    "# print image, text, price shape\n",
    "print('Before')\n",
    "print('image shape: ', image.shape)\n",
    "print('text shape: ', text.shape)\n",
    "print('price shape: ', price.shape)\n",
    "print('transaction shape: ', transaction.shape)\n",
    "print('')\n",
    "\n",
    "\"\"\"\n",
    "Keep only the items that appear in the inter\n",
    "\"\"\"\n",
    "# for dataset image, text, price, filter rows whose indices are in item_unique\n",
    "item_unique = np.unique(item)\n",
    "image = image.loc[image.index.isin(item_unique)]\n",
    "text = text.loc[text.index.isin(item_unique)]\n",
    "price = price.loc[price.index.isin(item_unique)]\n",
    "transaction = transaction.loc[transaction.index.isin(item_unique)]\n",
    "\n",
    "\"\"\"\n",
    "Change the item index to start from 0\n",
    "\"\"\"\n",
    "# convert indices using mapping_i\n",
    "image.index = image.index.map(mapping_i)\n",
    "text.index = text.index.map(mapping_i)\n",
    "price.index = price.index.map(mapping_i)\n",
    "transaction.index = transaction.index.map(mapping_i)\n",
    "\n",
    "# print image, text, price shape\n",
    "print('After')\n",
    "print('image shape: ', image.shape)\n",
    "print('text shape: ', text.shape)\n",
    "print('price shape: ', price.shape)\n",
    "print('transaction shape: ', transaction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert that the indices of image, text, price are the same, regardless of the order\n",
    "assert np.array_equal(np.sort(image.index.values), np.sort(text.index.values))\n",
    "assert np.array_equal(np.sort(image.index.values), np.sort(price.index.values))\n",
    "assert np.array_equal(np.sort(image.index.values), np.sort(transaction.index.values))\n",
    "\n",
    "# save df as npy file\n",
    "np.save(save_path+'image_feat.npy', image)\n",
    "np.save(save_path+'text_feat.npy', text)\n",
    "np.save(save_path+'price_feat.npy', price)\n",
    "np.save(save_path+'transaction_feat.npy', transaction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecBole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "392ba86a58589ad9d3867145c86eecd11f6e0889a5aad62cbef3708cb822e1d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
